{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import pprint\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "pp = pprint.PrettyPrinter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "DATA_FOLDER = '/home/xiaozhouzou/datasets/bank_additional_data/bank-additional'\n",
    "DATA_NAME = 'bank-additional-full.csv'\n",
    "DATA_PATH = os.path.join(DATA_FOLDER, DATA_NAME)\n",
    "\n",
    "RECOMBINE = True\n",
    "\n",
    "NUM_Y0_PART = 10\n",
    "\n",
    "NUM_BINS_HIST = 100\n",
    "\n",
    "STD_INCLUDE = 2\n",
    "\n",
    "REMOVE_NUMERICAL_OUTLIER = False\n",
    "\n",
    "VARIANCE_THRESHOLD = 0.1\n",
    "\n",
    "EPSILON = 0.01\n",
    "\n",
    "TRAIN_TEST_SPLIT = 0.25\n",
    "\n",
    "RF_PARAMS = {\"n_estimators\":100, \"min_samples_leaf\":2}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# part I: Read Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_data(data_path):\n",
    "\n",
    "    data = pd.read_csv(data_path, sep=\";\")\n",
    "\n",
    "    data[\"y\"] = pd.DataFrame(data['y'].map({\"no\":0, \"yes\":1}))\n",
    "    duration = data[\"duration\"]\n",
    "    data = data.drop(labels=[\"duration\"], axis=1)\n",
    "    data_features, data_target = data_preparation(data)\n",
    "    data = pd.concat([data_features, data_target], axis=1)\n",
    "\n",
    "    if RECOMBINE:\n",
    "\n",
    "        data_y0 = data.loc[data[\"y\"] == 0]\n",
    "        data_y1 = data.loc[data[\"y\"] == 1]\n",
    "\n",
    "        y1_rec_num = data_y1.shape[0]\n",
    "\n",
    "        recomb_data = []\n",
    "\n",
    "        for i in range(NUM_Y0_PART):\n",
    "\n",
    "            y0_data_sample = data_y0.sample(n=y1_rec_num)\n",
    "            sample_i = pd.concat([y0_data_sample, data_y1], axis=0)\n",
    "            sample_i = shuffle(sample_i)\n",
    "            sample_target = sample_i[\"y\"]\n",
    "            sample_features = sample_i.drop(labels=[\"y\"], axis=1)\n",
    "            recomb_data.append({\"features\":sample_features, \"target\":sample_target})\n",
    "        \n",
    "        return recomb_data, data_features, data_target\n",
    "            \n",
    "    else:\n",
    "        \n",
    "        return {\"features\": data_features, \"target\": data_target}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def data_preparation(data):\n",
    "    \n",
    "    data_target = data[\"y\"]\n",
    "    data_features = data.drop(labels=[\"y\"], axis=1)\n",
    "    \n",
    "    # get datatype information and histogram of all columns\n",
    "    column_info = get_column_info(data_features, data_target, vis=False)\n",
    "    \n",
    "    # transform categorical data to dummy variables\n",
    "    categorical_column_map, data_features = categorical_to_dummy(column_info, data_features)\n",
    "    \n",
    "    # drop the features valued unknown\n",
    "    data_features = drop_unknown_valued_features(data_features, categorical_column_map)\n",
    "\n",
    "    if REMOVE_NUMERICAL_OUTLIER:\n",
    "        # remove outliers for numerical features\n",
    "        data_features, data_target = numerical_outlier_removal(data_features, \n",
    "                                                               data_target, \n",
    "                                                               column_info, \n",
    "                                                               vis=False)\n",
    "    \n",
    "    return data_features, data_target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The f1 score for batch data is 0.8747736873868438\n",
      "The 0 model's f1 score on whole data is 0.3957414269748838\n",
      "The f1 score for batch data is 0.8831866328509621\n",
      "The 1 model's f1 score on whole data is 0.3777985587766104\n",
      "The f1 score for batch data is 0.88477058735615\n",
      "The 2 model's f1 score on whole data is 0.39013242704793155\n",
      "The f1 score for batch data is 0.8715167946979968\n",
      "The 3 model's f1 score on whole data is 0.4548972908603751\n",
      "The f1 score for batch data is 0.8879655735272296\n",
      "The 4 model's f1 score on whole data is 0.3837561217190337\n",
      "The f1 score for batch data is 0.8879606146501566\n",
      "The 5 model's f1 score on whole data is 0.3885521100489625\n",
      "The f1 score for batch data is 0.8866040828490538\n",
      "The 6 model's f1 score on whole data is 0.4150751209574739\n",
      "The f1 score for batch data is 0.8854182309656103\n",
      "The 7 model's f1 score on whole data is 0.39328707821858505\n",
      "The f1 score for batch data is 0.8930817610062892\n",
      "The 8 model's f1 score on whole data is 0.3820969281743135\n",
      "The f1 score for batch data is 0.8877990786149502\n",
      "The 9 model's f1 score on whole data is 0.3952728784369618\n",
      "The baseline f1 score on the training set is 0.006866952789699571\n",
      "The baseline f1 score on the test set is 0.0017137960582690659\n"
     ]
    }
   ],
   "source": [
    "if RECOMBINE:\n",
    "    \n",
    "    recombine_features = {}\n",
    "    \n",
    "    recomb_data, data_features, data_target = load_data(DATA_PATH)\n",
    "\n",
    "    for ind, batch_data in enumerate(recomb_data):\n",
    "        \n",
    "        batch_features = batch_data[\"features\"]\n",
    "        batch_target = batch_data[\"target\"]\n",
    "        \n",
    "        batch_features, selected_features = low_variance_filter(batch_features)\n",
    "\n",
    "        X_train, X_test, y_train, y_test = train_test_split(batch_features, \n",
    "                                                            batch_target, \n",
    "                                                            test_size=TRAIN_TEST_SPLIT,\n",
    "                                                            stratify=batch_target)\n",
    "        \n",
    "        model, features_mean, features_var = RandomForest_Predict(X_train, \n",
    "                                                                  y_train, \n",
    "                                                                  RF_PARAMS)\n",
    "        \n",
    "        model_features = data_features.loc[:, selected_features]\n",
    "        model_features = (model_features - features_mean) / np.sqrt(features_var + 1)\n",
    "        \n",
    "        model_pred = model.predict(model_features)\n",
    "        \n",
    "        model_f1_score = f1_score(y_true=data_target, y_pred=model_pred)\n",
    "        \n",
    "        recombine_features[\"batch_feature_\" + str(ind)] = model.predict(model_features)\n",
    "    \n",
    "        print(\"The \" + str(ind) + \" model\\'s f1 score on whole data is \" + \n",
    "              str(model_f1_score))\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(pd.DataFrame(recombine_features), \n",
    "                                                        data_target, \n",
    "                                                        test_size=TRAIN_TEST_SPLIT,\n",
    "                                                        stratify=data_target)\n",
    "    RandomForest_Baseline(X_train, y_train, X_test, y_test, RF_PARAMS)\n",
    "    \n",
    "else:\n",
    "    \n",
    "    data = load_data(DATA_PATH)\n",
    "    data_features = data[\"features\"]\n",
    "    data_target = data[\"target\"]\n",
    "    \n",
    "    data_features, selected_features = low_variance_filter(data_features)\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(data_features, \n",
    "                                                        data_target, \n",
    "                                                        test_size=TRAIN_TEST_SPLIT,\n",
    "                                                        stratify=data_target)\n",
    "    \n",
    "#     # get the baseline f1-score for RandomForest\n",
    "#     RandomForest_Baseline(X_train, y_train, X_test, y_test, RF_PARAMS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# part II: Pre-processing and Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_column_info(data_features, data_target, vis=False, verbose=False):\n",
    "    \n",
    "    columns = list(data_features.columns.values)\n",
    "    column_dtype_dict = {}\n",
    "\n",
    "    for column_name in columns:\n",
    "        column_dtype_dict[column_name] = data_features[column_name].dtype\n",
    "    \n",
    "    if verbose:\n",
    "        \n",
    "        pp.pprint(\"Column Information:\")\n",
    "        pp.pprint(column_dtype_dict)\n",
    "    \n",
    "    if vis:\n",
    "        \n",
    "        pp.pprint(\"Histogram of Columns:\")\n",
    "    \n",
    "        for column_name, column_dtype in column_dtype_dict.items():\n",
    "            \n",
    "            title = \"histogram of \" + column_name\n",
    "            \n",
    "            if column_dtype == \"object\":\n",
    "                \n",
    "                data_features[column_name].value_counts().plot(kind='bar', title=title)\n",
    "                plt.show()\n",
    "                \n",
    "            else: # dtype is int64 or float64\n",
    "                \n",
    "                data_features[column_name].plot.hist(title=title, bins=NUM_BINS_HIST)\n",
    "                plt.show()\n",
    "                \n",
    "        data_target.plot.hist(bins=NUM_BINS_HIST)\n",
    "    \n",
    "    return column_dtype_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## map the categorical values to dummy variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def categorical_to_dummy(column_dtype_dict, data_features, verbose=False):\n",
    "    \n",
    "    categorical_features = [k for (k,v) in column_dtype_dict.items() if v == \"object\"]\n",
    "    column_map_dict = {}\n",
    "\n",
    "    for col in categorical_features:\n",
    "        \n",
    "        unique_val_list = list(data_features[col].unique())\n",
    "        column_map_dict[col] = dict(zip(unique_val_list, list(range(len(unique_val_list)))))\n",
    "        data_features[col] = data_features[col].map(column_map_dict[col])\n",
    "    \n",
    "    if verbose:\n",
    "        pp.pprint(column_map_dict)\n",
    "\n",
    "    data_features = pd.get_dummies(data_features, columns=categorical_features)\n",
    "    \n",
    "    return column_map_dict, data_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## drop the features valued unknown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def drop_unknown_valued_features(features, column_map_dict):\n",
    "\n",
    "    for col, map_dict in column_map_dict.items():\n",
    "        \n",
    "        for col_val, val_code in map_dict.items():\n",
    "            \n",
    "            if col_val == \"unknown\":\n",
    "                \n",
    "                drop_col = col + \"_\" + str(val_code)\n",
    "                features.drop(drop_col, axis=1, inplace=True)\n",
    "\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## outlier removal by empirical mean and standard deviation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def numerical_outlier_removal(data, target, column_dtype_dict, vis=False, verbose=False):\n",
    "\n",
    "    numerical_features = [k for (k,v) in column_dtype_dict.items() \n",
    "                          if v == \"int64\" or v == \"float64\"]    \n",
    "\n",
    "    for col in numerical_features:\n",
    "\n",
    "        col_mean = data[col].mean()\n",
    "        col_std = data[col].std()\n",
    "        col_selected = data[col].between(col_mean - STD_INCLUDE * col_std, \n",
    "                                         col_mean + STD_INCLUDE * col_std, \n",
    "                                         inclusive=True)\n",
    "        selected_ratio = col_selected.value_counts()[True] / col_selected.size\n",
    "\n",
    "        if verbose:\n",
    "            print(numerical_features)\n",
    "            print(\"selected ratio for feature {} is {}\".format(col, selected_ratio))\n",
    "\n",
    "        if selected_ratio > 0.9:\n",
    "        # The outliers can't be greater than 10%\n",
    "\n",
    "            data = data.loc[col_selected]\n",
    "            target = target.loc[col_selected]\n",
    "\n",
    "            if vis:\n",
    "\n",
    "                data[col].plot.hist(bins=NUM_BINS_HIST)\n",
    "                plt.show()\n",
    "\n",
    "    return data, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## remove the features with too little variance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def low_variance_filter(features, verbose=False):\n",
    "    \n",
    "    feature_series = pd.Series(features.columns)\n",
    "    \n",
    "    feature_sel_VT = VarianceThreshold(threshold=VARIANCE_THRESHOLD)\n",
    "    feature_sel_VT.fit(features)\n",
    "    VTed_features = feature_sel_VT.transform(features)\n",
    "    \n",
    "    selected_feature_indices = feature_sel_VT.get_support(indices=True)\n",
    "    selected_features = feature_series.loc[selected_feature_indices]\n",
    "\n",
    "    if verbose:\n",
    "        pp.pprint(selected_features)\n",
    "    \n",
    "    return VTed_features, selected_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cv = StratifiedKFold(n_splits=3, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def zScoreNormalizer(features_train):\n",
    "\n",
    "    normalizer_zScore = StandardScaler(with_mean=True, with_std=True)\n",
    "    normalizer_zScore.fit(features_train)\n",
    "    features_train = normalizer_zScore.transform(features_train)\n",
    "    \n",
    "    return features_train, normalizer_zScore.mean_, normalizer_zScore.var_, normalizer_zScore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def RandomForest_Baseline(features_train, target_train, features_test, target_test, \n",
    "                          RF_param_dict):\n",
    "    \n",
    "    features_train, _, _, normalizer_zScore = zScoreNormalizer(features_train)\n",
    "    \n",
    "    baseline_clf = RandomForestClassifier(n_estimators=RF_param_dict[\"n_estimators\"], \n",
    "                                          min_samples_leaf=RF_param_dict[\"min_samples_leaf\"])\n",
    "    baseline_clf.fit(features_train, target_train)\n",
    "    \n",
    "    baseline_train_f1 = f1_score(y_true=target_train, \n",
    "                                 y_pred=baseline_clf.predict(features_train),)\n",
    "    print(\"The baseline f1 score on the training set is {}\".format(baseline_train_f1))\n",
    "\n",
    "    features_test_zNormed = normalizer_zScore.transform(features_test)\n",
    "    baseline_test_f1 = f1_score(y_true=target_test, \n",
    "                                y_pred=baseline_clf.predict(features_test_zNormed),)\n",
    "    print(\"The baseline f1 score on the test set is {}\".format(baseline_test_f1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def RandomForest_Predict(features, target, RF_param_dict):\n",
    "\n",
    "    features, features_mean, features_var, _ = zScoreNormalizer(features)\n",
    "    \n",
    "    RF_clf = RandomForestClassifier(n_estimators=RF_param_dict[\"n_estimators\"], \n",
    "                                    min_samples_leaf=RF_param_dict[\"min_samples_leaf\"])\n",
    "    RF_clf.fit(features, target)\n",
    "    data_f1_score = f1_score(y_true=target, y_pred=RF_clf.predict(features))\n",
    "    \n",
    "    print(\"The f1 score for batch data is \" + str(data_f1_score))\n",
    "    \n",
    "    return RF_clf, features_mean, features_var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
